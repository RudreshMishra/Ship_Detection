{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 4\nEDGE_CROP = 16\nNB_EPOCHS = 5\nGAUSSIAN_NOISE = 0.1\nUPSAMPLE_MODE = 'SIMPLE'\n# downsampling inside the network\nNET_SCALING = None\n# downsampling in preprocessing\nIMG_SCALING = (1, 1)\n# number of validation images to use\nVALID_IMG_COUNT = 400\n# maximum number of steps_per_epoch in training\nMAX_TRAIN_STEPS = 200\nAUGMENT_BRIGHTNESS = False","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom skimage.io import imread\nimport matplotlib.pyplot as plt\nfrom skimage.segmentation import mark_boundaries\nfrom skimage.util import montage as montage\nmontage_rgb = lambda x: np.stack([montage(x[:, :, :, i]) for i in range(x.shape[3])], -1)\nship_dir = '../input'\ntrain_image_dir = os.path.join(ship_dir, 'train_v2')\ntest_image_dir = os.path.join(ship_dir, 'test_v2')\nimport gc; gc.enable() # memory is tight","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"TO convert the images into the runlength encoding or to decode them back.Also to mask the image based on the value of runlength encoding.\nReference :-: https://www.kaggle.com/paulorzp/run-length-encode-and-decode"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from skimage.morphology import label\ndef multi_rle_encode(img):\n    labels = label(img[:, :, 0])\n    return [rle_encode(labels==k) for k in np.unique(labels[labels>0])]\n\n\ndef rle_encode(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef rle_decode(mask_rle, shape=(768, 768)):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n   # print(starts)\n    ends = starts + lengths\n    #print(ends)\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape).T  # Needed to align to RLE direction\n\ndef masks_as_image(in_mask_list):\n    # Take the individual ship masks and create a single mask array for all ships\n    all_masks = np.zeros((768, 768), dtype = np.int16)\n    for mask in in_mask_list:\n        if isinstance(mask, str):\n            all_masks += rle_decode(mask)\n    return np.expand_dims(all_masks, -1)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"masks = pd.read_csv(os.path.join('../input',\n                                 'train_ship_segmentations_v2.csv'))\nprint(masks.shape[0], 'masks found')\nprint(masks['ImageId'].value_counts().shape[0])\nmasks.head()","execution_count":4,"outputs":[{"output_type":"stream","text":"231723 masks found\n192556\n","name":"stdout"},{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"         ImageId                                      EncodedPixels\n0  00003e153.jpg                                                NaN\n1  0001124c7.jpg                                                NaN\n2  000155de5.jpg  264661 17 265429 33 266197 33 266965 33 267733...\n3  000194a2d.jpg  360486 1 361252 4 362019 5 362785 8 363552 10 ...\n4  000194a2d.jpg  51834 9 52602 9 53370 9 54138 9 54906 9 55674 ...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ImageId</th>\n      <th>EncodedPixels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00003e153.jpg</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0001124c7.jpg</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000155de5.jpg</td>\n      <td>264661 17 265429 33 266197 33 266965 33 267733...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>000194a2d.jpg</td>\n      <td>360486 1 361252 4 362019 5 362785 8 363552 10 ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>000194a2d.jpg</td>\n      <td>51834 9 52602 9 53370 9 54138 9 54906 9 55674 ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"masks['ships'] = masks['EncodedPixels'].map(lambda c_row: 1 if isinstance(c_row, str) else 0)\nunique_img_ids = masks.groupby('ImageId').agg({'ships': 'sum'}).reset_index()\nunique_img_ids['has_ship'] = unique_img_ids['ships'].map(lambda x: 1.0 if x>0 else 0.0)\nunique_img_ids['has_ship_vec'] = unique_img_ids['has_ship'].map(lambda x: [x])\n# calculate the size of the images as some files are too small/corrupt we need to ignore them\nunique_img_ids['file_size_kb'] = unique_img_ids['ImageId'].map(lambda c_img_id: \n                                                               os.stat(os.path.join(train_image_dir, \n                                                                                    c_img_id)).st_size/1024)\nunique_img_ids = unique_img_ids[unique_img_ids['file_size_kb']>50] # keep only 50kb files\nunique_img_ids['file_size_kb'].hist()\nmasks.drop(['ships'], axis=1, inplace=True)\nunique_img_ids.sample(5)","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"              ImageId  ships  has_ship has_ship_vec  file_size_kb\n142959  be12550f4.jpg      0       0.0        [0.0]    168.923828\n17953   17db1b3a9.jpg      0       0.0        [0.0]    103.409180\n62863   53be9c6db.jpg      0       0.0        [0.0]    193.915039\n27748   24e73aeac.jpg      0       0.0        [0.0]     99.054688\n83318   6eaab8529.jpg      0       0.0        [0.0]    129.140625","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ImageId</th>\n      <th>ships</th>\n      <th>has_ship</th>\n      <th>has_ship_vec</th>\n      <th>file_size_kb</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>142959</th>\n      <td>be12550f4.jpg</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>[0.0]</td>\n      <td>168.923828</td>\n    </tr>\n    <tr>\n      <th>17953</th>\n      <td>17db1b3a9.jpg</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>[0.0]</td>\n      <td>103.409180</td>\n    </tr>\n    <tr>\n      <th>62863</th>\n      <td>53be9c6db.jpg</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>[0.0]</td>\n      <td>193.915039</td>\n    </tr>\n    <tr>\n      <th>27748</th>\n      <td>24e73aeac.jpg</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>[0.0]</td>\n      <td>99.054688</td>\n    </tr>\n    <tr>\n      <th>83318</th>\n      <td>6eaab8529.jpg</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>[0.0]</td>\n      <td>129.140625</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(masks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nfrom sklearn.model_selection import train_test_split\ntrain_ids, valid_ids = train_test_split(unique_img_ids, \n                 test_size = 0.3, \n                 stratify = unique_img_ids['ships'])\ntrain_df = pd.merge(masks, train_ids)\nvalid_df = pd.merge(masks, valid_ids)\nprint(train_df.shape[0], 'training masks')\nprint(valid_df.shape[0], 'validation masks')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['grouped_ship_count'] = train_df['ships'].map(lambda x: (x+1)//2).clip(0, 7)\n# 1500*7(for the images) + 500 for the non images\ndef sample_ships(in_df, base_rep_val=1500):\n    if in_df['ships'].values[0]==0:\n        return in_df.sample(base_rep_val//3) # even more strongly undersample no ships\n    else:\n        return in_df.sample(base_rep_val, replace=(in_df.shape[0]<base_rep_val))\n    \nbalanced_train_df = train_df.groupby('grouped_ship_count').apply(sample_ships)\nbalanced_train_df['ships'].hist(bins=np.arange(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(balanced_train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_image_gen(in_df, batch_size = BATCH_SIZE):\n    all_batches = list(in_df.groupby('ImageId'))\n    out_rgb = []\n    out_mask = []\n    while True:\n        np.random.shuffle(all_batches)\n        for c_img_id, c_masks in all_batches:\n            rgb_path = os.path.join(train_image_dir, c_img_id)\n            c_img = imread(rgb_path)\n            c_mask = masks_as_image(c_masks['EncodedPixels'].values)\n            if IMG_SCALING is not None:\n                c_img = c_img[::IMG_SCALING[0], ::IMG_SCALING[1]]\n                c_mask = c_mask[::IMG_SCALING[0], ::IMG_SCALING[1]]\n            out_rgb += [c_img]\n            out_mask += [c_mask]\n            if len(out_rgb)>=batch_size:\n                yield np.stack(out_rgb, 0)/255.0, np.stack(out_mask, 0)\n                out_rgb, out_mask=[], []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_x, valid_y = next(make_image_gen(valid_df, VALID_IMG_COUNT))\nprint(valid_x.shape, valid_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\ndg_args = dict(featurewise_center = False, \n                  samplewise_center = False,\n                  rotation_range = 15, \n                  width_shift_range = 0.1, \n                  height_shift_range = 0.1, \n                  shear_range = 0.01,\n                  zoom_range = [0.9, 1.25],  \n                  horizontal_flip = True, \n                  vertical_flip = True,\n                  fill_mode = 'reflect',\n                   data_format = 'channels_last')\n# brightness can be problematic since it seems to change the labels differently from the images \nif AUGMENT_BRIGHTNESS:\n    dg_args[' brightness_range'] = [0.5, 1.5]\nimage_gen = ImageDataGenerator(**dg_args)\n\nif AUGMENT_BRIGHTNESS:\n    dg_args.pop('brightness_range')\nlabel_gen = ImageDataGenerator(**dg_args)\n\ndef create_aug_gen(in_gen, seed = None):\n    np.random.seed(seed if seed is not None else np.random.choice(range(9999)))\n    for in_x, in_y in in_gen:\n        seed = np.random.choice(range(9999))\n        # keep the seeds syncronized otherwise the augmentation to the images is different from the masks\n        g_x = image_gen.flow(255*in_x, \n                             batch_size = in_x.shape[0], \n                             seed = seed, \n                             shuffle=True)\n        g_y = label_gen.flow(in_y, \n                             batch_size = in_x.shape[0], \n                             seed = seed, \n                             shuffle=True)\n\n        yield next(g_x)/255.0, next(g_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def conv_block(filters,  strides=(2, 2)):\n    \n    def layer(input_tensor):\n        x = BatchNormalization()(input_tensor)\n        x = Activation('relu')(x)\n        shortcut = x\n        x = ZeroPadding2D(padding=(1, 1))(x)\n        x = Conv2D(filters, (3, 3), strides=strides)(x)\n\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n        x = ZeroPadding2D(padding=(1, 1))(x)\n        x = Conv2D(filters, (3, 3))(x)\n\n        shortcut = Conv2D(filters, (1, 1), strides=strides)(shortcut)\n        x = Add()([x, shortcut])\n        return x\n    return layer\n\n\ndef identity_block(filters):\n\n    def layer(input_tensor):\n        x = BatchNormalization()(input_tensor)\n        x = Activation('relu')(x)\n        x = ZeroPadding2D(padding=(1, 1))(x)\n        x = Conv2D(filters, (3, 3))(x)\n\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n        x = ZeroPadding2D(padding=(1, 1))(x)\n        x = Conv2D(filters, (3, 3))(x)\n\n        x = Add()([x, input_tensor])\n        return x\n\n    return layer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def up_block(filters, kernel_size=(3,3), upsample_rate=(2,2),\n                     batchnorm=False, skip=None):\n\n    def layer(input_tensor):\n\n        x = UpSampling2D(size=upsample_rate)(input_tensor)\n\n        if skip is not None:\n            x = Concatenate()([x, skip])\n\n        x = Conv2D(filters, kernel_size, padding='same')(x)\n        if batchnorm:\n            x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n\n        x = Conv2D(filters, kernel_size, padding='same')(x)\n        if batchnorm:\n            x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n\n        return x\n    return layer\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras.backend as K\nfrom keras.layers import Input\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Activation\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.layers import ZeroPadding2D\nfrom keras.layers import Dense\nfrom keras.models import Model\nfrom keras.engine import get_source_inputs\nfrom keras.layers import Add\nfrom keras.layers import UpSampling2D\nfrom keras.layers import Concatenate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def UResNet34(input_shape=(None, None, 3), classes=1, decoder_filters=16,  include_top=True, input_tensor=None):\n    from keras_applications.imagenet_utils import _obtain_input_shape\n    input_shape = _obtain_input_shape(input_shape,\n                                      default_size=224,\n                                      min_size=197,\n                                      data_format='channels_last',\n                                      require_flatten=include_top)\n    if input_tensor is None:\n        img_input = Input(shape=input_shape, name='data')\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n            \n    # resnet bottom\n    x = BatchNormalization()(img_input)\n    x = ZeroPadding2D(padding=(3, 3))(x)\n    x = Conv2D(64, (7, 7), strides=(2, 2), name='conv0')(x)\n    x = BatchNormalization(name='bn0')(x)\n    x = Activation('relu', name='relu0')(x)\n    x = ZeroPadding2D(padding=(1, 1))(x)\n    x = MaxPooling2D((3, 3), strides=(2, 2), padding='valid', name='pooling0')(x)\n    \n    #resent model \n    x = conv_block(64,strides=(1, 1))(x)\n    x = identity_block(64)(x)\n    \n    x = conv_block(128, strides=(2, 2))(x)\n    x = identity_block(128)(x)\n    \n    x = conv_block(128, strides=(2, 2))(x)\n    x = identity_block(128)(x)\n    \n    x = conv_block(256, strides=(2, 2))(x)\n    x = identity_block(256)(x)\n    \n    x = conv_block(256, strides=(2, 2))(x)\n    x = identity_block(256)(x)\n    \n    x = conv_block(512, strides=(2, 2))(x)\n    x = identity_block(512)(x)\n    \n    x = conv_block(512, strides=(2, 2))(x)\n    x = identity_block(512)(x)\n    \n    x = BatchNormalization()(x)\n    x = Activation('relu', name='relu1')(x)\n    \n\n    backbone = Model(img_input, x)\n    backbone.layers\n    #unet model\n    #skip = None\n    skip = backbone.layers[120].output\n    x = up_block(256, upsample_rate=(2,2), skip=skip)(x)\n    skip = None\n    x = up_block(128, upsample_rate=(2,2), skip=skip)(x)\n    skip = backbone.layers[68].output\n    x = up_block(128, upsample_rate=(2,2), skip=skip)(x)\n    skip = None\n    x = up_block(64, upsample_rate=(2,2), skip=skip)(x)\n    skip = backbone.layers[30].output\n    x = up_block(64, upsample_rate=(2,2), skip=skip)(x)\n    skip = None\n    x = up_block(32, upsample_rate=(2,2), skip=skip)(x)\n    skip = backbone.layers[4].output\n    x = up_block(32, upsample_rate=(2,2), skip=skip)(x)\n    skip = None\n    x = up_block(16, upsample_rate=(2,2), skip=skip)(x)\n    x = Conv2D(1, (3,3), padding='same', name='final_conv')(x)\n\n    activation = 'sigmoid'\n    x = Activation(activation, name=activation)(x)\n    model = Model(img_input, x)\n    return model\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seg_model = UResNet34(input_shape=(768,768,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(seg_model.layers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seg_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras.backend as K\nfrom keras.optimizers import Adam\nfrom keras.losses import binary_crossentropy\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dice_loss(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = y_true_f * y_pred_f\n    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return 1. - score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bce_logdice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) - K.log(1. - dice_loss(y_true, y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dice_coef(y_true, y_pred, smooth=1):\n    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n    return K.mean( (2. * intersection + smooth) / (union + smooth), axis=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dice_p_bce(in_gt, in_pred):\n    return 1e-3*binary_crossentropy(in_gt, in_pred) - dice_coef(in_gt, in_pred)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def true_positive_rate(y_true, y_pred):\n    return K.sum(K.flatten(y_true)*K.flatten(K.round(y_pred)))/K.sum(y_true)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nweight_path=\"{}_weights.best.hdf5\".format('seg_model')\n\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_dice_coef', verbose=1, \n                             save_best_only=True, mode='max', save_weights_only = True)\n\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_dice_coef', factor=0.5, \n                                   patience=3, \n                                   verbose=1, mode='max', epsilon=0.0001, cooldown=2, min_lr=1e-6)\nearly = EarlyStopping(monitor=\"val_dice_coef\", \n                      mode=\"max\", \n                      patience=15) # probably needs to be more patient, but kaggle time is limited\ncallbacks_list = [checkpoint, early, reduceLROnPlat]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seg_model.compile(optimizer=Adam(1e-4, decay=1e-6), loss=bce_logdice_loss, metrics=[dice_coef, 'binary_accuracy', true_positive_rate])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nepochs = 100\nbatch_size = 32\n\nstep_count = min(MAX_TRAIN_STEPS, balanced_train_df.shape[0]//BATCH_SIZE)\naug_gen = create_aug_gen(make_image_gen(balanced_train_df))\nloss_history = [seg_model.fit_generator(aug_gen,\n                            steps_per_epoch=step_count,\n                            validation_data=(valid_x, valid_y), \n                            epochs=10,\n                            callbacks=callbacks_list,shuffle=True)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seg_model.load_weights(weight_path)\nseg_model.save('seg_model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_paths = os.listdir(test_image_dir)\nprint(len(test_paths), 'test images found')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, m_axs = plt.subplots(20, 2, figsize = (10, 40))\n[c_ax.axis('off') for c_ax in m_axs.flatten()]\nfor (ax1, ax2), c_img_name in zip(m_axs, test_paths):\n    c_path = os.path.join(test_image_dir, c_img_name)\n    c_img = imread(c_path)\n    first_img = np.expand_dims(c_img, 0)/255.0\n    first_seg = seg_model.predict(first_img)\n    ax1.imshow(first_img[0])\n    ax1.set_title('Image')\n    ax2.imshow(first_seg[0,:, :, 0])\n    ax2.set_title('Prediction')\nfig.savefig('test_predictions.png')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}